# CSV Data Cleaning & Analysis Tool

A full-stack application for Data Engineers and AI Engineers to clean, validate, and analyze CSV data. Simple to start with, scalable as your needs grow.

## Features

âœ¨ **Data Upload**
- Drag-and-drop CSV file upload
- Automatic data parsing and basic statistics
- **Multi-encoding support** (UTF-8, Latin-1, ISO-8859-1, CP1252, UTF-16)

ğŸ“Š **Data Analysis**
- Dataset overview (rows, columns, memory usage)
- Column-level statistics
- Data quality scoring (completeness & uniqueness)
- Missing values detection
- Duplicate row detection
- Interactive visualizations (column distributions, correlations)

ğŸ§¹ **Data Cleaning**
- **Logical operation order**: Optimized workflow for efficient cleaning
  1. **Drop Columns** - Select which columns to keep
  2. **Fill Missing Values** - Handle NaN values (mean, median, forward fill, empty string, or drop)
  3. **Clean String Values** - Trim extra spaces and normalize whitespace in text columns
  4. **Data Standardization & Normalization** - Choose one numeric scaling method (z-score or min-max)
  5. **Remove Duplicates** - Remove duplicate rows
  6. **Remove Outliers** - Optional IQR-based outlier removal
- Multiple cleaning iterations supported
- Real-time preview updates after each operation
- Real-time quality score updates

â¬‡ï¸ **Export**
- Download cleaned datasets as CSV

ğŸ–¥ï¸ **Desktop Application**
- Standalone Windows executable (.exe)
- No installation required
- Built-in web UI with native window
- All features available offline

## Tech Stack

### Backend
- **Python 3.9+**
- **FastAPI** - Modern web framework
- **Pandas** - Data manipulation and analysis
- **NumPy** - Numerical computing

### Frontend
- **React 18** - UI framework
- **Recharts** - Data visualization library
- **Axios** - HTTP client
- **CSS3** - Styling

## Project Structure

```
test_application/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ data_routes.py       # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaner.py           # Data cleaning logic
â”‚   â”‚   â”‚   â””â”€â”€ analyzer.py          # Data analysis logic
â”‚   â”‚   â””â”€â”€ main.py                  # FastAPI app initialization
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â””â”€â”€ .gitignore
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.js        # File upload component
â”‚   â”‚   â”‚   â”œâ”€â”€ DataAnalysis.js      # Analysis display component
â”‚   â”‚   â”‚   â””â”€â”€ DataCleaner.js       # Cleaning interface component
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js               # API client
â”‚   â”‚   â”œâ”€â”€ App.js                   # Main app component
â”‚   â”‚   â”œâ”€â”€ App.css                  # App styling
â”‚   â”‚   â””â”€â”€ index.js                 # React entry point
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .gitignore
â”œâ”€â”€ main_win.py                       # Desktop app entry point
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Getting Started

### Prerequisites

- Python 3.9 or higher
- Node.js 16 or higher
- npm or yarn

### Backend Setup

1. Navigate to the backend directory:
```bash
cd backend
```

2. Create a virtual environment:
```bash
python -m venv venv
```

3. Activate the virtual environment:
```bash
# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

4. Install dependencies:
```bash
pip install -r requirements.txt
```

5. Start the FastAPI server:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`
- Interactive API docs: `http://localhost:8000/docs`

### Frontend Setup

1. Navigate to the frontend directory:
```bash
cd frontend
```

2. Install dependencies:
```bash
npm install
```

3. Start the development server:
```bash
npm start
```

The application will open at `http://localhost:3000`

## Desktop Application (Windows)

### Option 1: Use Pre-built Executable

Simply download and run `csv-data-tool.exe` - no installation required!

### Option 2: Build from Source

1. Build the frontend:
```bash
cd frontend
npm install
npm run build
```

2. Activate the backend virtual environment:
```bash
cd ../backend
venv\Scripts\activate  # Windows
```

3. Build the executable:
```bash
cd ..
pyinstaller --noconfirm --onefile --add-data "frontend\build;frontend\build" --add-data "backend\app;app" --name csv-data-tool main_win.py
```

4. Run the executable:
```bash
.\dist\csv-data-tool.exe
```

The application will open in a native window with the full web interface embedded.

## API Endpoints

### Upload File
**POST** `/api/data/upload`
- Upload a CSV file for processing
- Returns: file_id, basic statistics

### Analyze Data
**GET** `/api/data/analyze/{file_id}`
- Get comprehensive analysis of uploaded data
- Returns: basic stats, column stats, quality score, missing values, duplicates

### Clean Data
**POST** `/api/data/clean/{file_id}`
- Apply cleaning operations to data (executed in logical order)
- Request Body:
  - `columns_to_drop` (array): List of column names to remove
  - `fill_missing` (string): Strategy for missing values ('mean', 'median', 'forward_fill', 'empty_string', 'drop')
  - `clean_strings` (bool): Trim and normalize whitespace in string/text columns
  - `standardize_data` (string): Numeric scaling method ('zscore' or 'minmax')
  - `remove_duplicates` (bool): Remove duplicate rows
  - `remove_outliers` (bool): Remove statistical outliers
- Operations are applied in sequence: columns â†’ missing â†’ strings â†’ scaling â†’ duplicates â†’ outliers
- Returns: Cleaned dataset info with new file_id (e.g., `{file_id}_cleaned_1`)

### Download Data
**GET** `/api/data/download/{file_id}`
- Download processed data as CSV
- Returns: CSV content

## Usage Example

1. **Launch the app**: Open `http://localhost:3000` in your browser
2. **Upload CSV**: Drag and drop or click to upload a CSV file
3. **Review Analysis**: Check data quality metrics, visualizations, and issues
4. **Clean Data**: Select cleaning options in logical order:
   - Step 1: Choose columns to keep (uncheck columns to drop)
   - Step 2: Select missing value strategy
  - Step 3: Optionally clean string values (trim/whitespace normalization)
  - Step 4: Optionally apply one scaling method (z-score or min-max)
  - Step 5: Check to remove duplicates
  - Step 6: Optionally remove outliers
5. **Apply Cleaning**: Click "Apply Cleaning" (can repeat multiple times)
6. **Review Results**: Check updated quality score and preview
7. **Download**: Export the cleaned dataset

### Why This Cleaning Order?

The operations are executed in a specific order for optimal results:

1. **Drop Columns First**: Reduces dataset size early, making subsequent operations faster
2. **Fill Missing Values**: Handles data completeness before deduplication
3. **Clean String Values**: Standardizes text formatting before record comparison and modeling
4. **Apply Numeric Scaling**: Makes numeric features comparable on a consistent scale
5. **Remove Duplicates**: Works on standardized values after text and numeric cleanup
6. **Remove Outliers Last**: Optional step that works best on cleaned, scaled data

This sequence minimizes computational overhead and produces the most reliable results.

## Scalability Features

This project is designed to scale:

### Short-term (v0.2-v0.3)
- Database integration (PostgreSQL) for persistent storage
- User authentication and file management
- Batch processing for large files
- More statistical analysis options
- Data visualization with charts

### Medium-term (v0.4-v0.5)
- REST API optimization
- Caching layer (Redis)
- Job queue for async processing (Celery)
- Multiple file format support (Excel, JSON, Parquet)
- Workflow/pipeline scheduling

### Long-term (v1.0+)
- Microservices architecture
- Distributed processing (Apache Spark)
- Real-time monitoring dashboards
- ML-based data quality detection
- Cloud deployment (AWS, GCP, Azure)

## Development Tips

### Adding New Features

1. **Backend**: Add new services in `app/services/` and routes in `app/api/`
2. **Frontend**: Create new components in `src/components/` with corresponding CSS

### Testing

```bash
# Backend - create tests in backend/tests/
pytest

# Frontend - create tests in frontend/src/
npm test
```

### Troubleshooting

**CORS Issues**: Already configured in backend for development
**Port Conflicts**: Change ports in startup commands if needed

## Future Enhancements

- [ ] Database integration for persistent data storage
- [ ] User authentication and profiles
- [ ] Advanced statistical analysis
- [x] Data visualization (charts, histograms) - âœ… Basic charts implemented
- [ ] More chart types (scatter plots, heatmaps)
- [ ] Workflow automation and scheduling
- [ ] Support for multiple file formats (Excel, JSON, Parquet)
- [ ] Real-time collaboration features
- [ ] API rate limiting and security
- [ ] Undo/redo functionality for cleaning operations

## Contributing

Feel free to extend this project! Areas for contribution:
- Additional cleaning algorithms
- UI/UX improvements
- Performance optimizations
- Testing and documentation

## License

MIT License - feel free to use this project for learning and development.

## Support

For issues or questions, check the code comments or extend with your own features!

---

**Version**: 0.3.0  
**Last Updated**: February 19, 2026

**Recent Updates (v0.3.0)**:
- âœ… **String cleaning step** - Added text normalization (trim + whitespace cleanup)
- âœ… **Data standardization/normalization step** - Added z-score and min-max scaling options
- âœ… **Expanded ordered pipeline** - Cleaning flow now has 6 explicit ordered steps
- âœ… **UI wording improvements** - Combined scaling terminology in one user-facing step

**Recent Updates (v0.2.0)**:
- âœ… **Column dropping feature** - Select which columns to keep in your dataset
- âœ… **Logical operation ordering** - Cleaning operations now execute in optimal sequence
- âœ… **Multiple cleaning iterations** - Apply cleaning operations multiple times with unique file IDs
- âœ… **Data visualizations** - Interactive charts for column distributions and correlations
- âœ… **Large dataset handling** - Optimized JSON serialization for datasets with 1M+ rows
- âœ… **UI/UX improvements** - Step-by-step cleaning workflow with clear descriptions

**Previous Updates (v0.1.0)**:
- âœ… Windows desktop application with PyInstaller
- âœ… Multi-encoding CSV support (handles international characters)
- âœ… Fixed API routing for packaged application
- âœ… Dynamic API URL resolution for dev/production
